{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lastest",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bururaru/Food-101-with-VGGNET16/blob/master/lastest_adam_10-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Orfif2uOYJBG",
        "colab_type": "code",
        "outputId": "ee62510c-c56a-4314-f543-7a6f2820f26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "# Check if GPU is enabled\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.test.gpu_device_name())\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n",
            "/device:GPU:0\n",
            "Fri May 24 00:57:30 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    30W /  70W |    221MiB / 15079MiB |      2%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMHQRIMZVWW4",
        "colab_type": "code",
        "outputId": "a5cd8a93-0f38-4f9a-caa8-dc4056850d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "  LOG_DIR = 'drive/data/tb_logs'\n",
        "\t\n",
        "  !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "  !unzip ngrok-stable-linux-amd64.zip\n",
        "\t\n",
        "  import os\n",
        "  if not os.path.exists(LOG_DIR):\n",
        "    os.makedirs(LOG_DIR)\n",
        "\t  \n",
        "  get_ipython().system_raw(\n",
        "      'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "      .format(LOG_DIR))\n",
        "\t\n",
        "  get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\t\n",
        "  !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "      \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-24 00:58:05--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 3.209.102.29, 34.232.40.183, 52.7.169.168, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|3.209.102.29|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16648024 (16M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  15.88M  6.13MB/s    in 2.6s    \n",
            "\n",
            "2019-05-24 00:58:09 (6.13 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [16648024/16648024]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://3ea11add.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_9PIjOPdNnw",
        "colab_type": "code",
        "outputId": "1f5615ba-d3d0-49af-c52d-5e318b52cce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWudoKZKdP7x",
        "colab_type": "code",
        "outputId": "a008544d-165a-4da4-ad20-5de34b051bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# shutil.copy(\"/gdrive/My Drive/best_model_101class_school.hdf5\", \"/content\")\n",
        "shutil.copy(\"/gdrive/My Drive/food-101.tar.gz\", \"/content\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/food-101.tar.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-T0qbj1YNf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Helper function to download data and extract\n",
        "# import os\n",
        "# def get_data_extract():\n",
        "#   if \"food-101\" in os.listdir():\n",
        "#     print(\"Dataset already exists\")\n",
        "#   else:\n",
        "#     print(\"Downloading the data...\")\n",
        "#     !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
        "#     print(\"Dataset downloaded!\")\n",
        "#     print(\"Extracting data..\")\n",
        "#     !tar xzvf food-101.tar.gz\n",
        "#     print(\"Extraction done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXGd5ybWYO79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xzvf food-101.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVF-wVRNYQPL",
        "colab_type": "code",
        "outputId": "b0a156cf-9522-4e47-852b-937dbd07c799",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the extracted dataset folder\n",
        "!ls food-101/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images\tlicense_agreement.txt  meta  README.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_RnwiyCYRAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.listdir('food-101/images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sY4vPWvYSC1",
        "colab_type": "code",
        "outputId": "87d466b8-bd1d-4638-ab13-4bb2b5594765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "os.listdir('food-101/meta')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.json',\n",
              " 'test.json',\n",
              " 'test.txt',\n",
              " 'classes.txt',\n",
              " 'train.txt',\n",
              " 'labels.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X14hcGuDYToq",
        "colab_type": "code",
        "outputId": "a16d3274-4a5f-47ac-b5c9-1e8a7a5c5902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!head food-101/meta/train.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apple_pie/1005649\n",
            "apple_pie/1014775\n",
            "apple_pie/1026328\n",
            "apple_pie/1028787\n",
            "apple_pie/1043283\n",
            "apple_pie/1050519\n",
            "apple_pie/1057749\n",
            "apple_pie/1057810\n",
            "apple_pie/1072416\n",
            "apple_pie/1074856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evU4ORPaYUiM",
        "colab_type": "code",
        "outputId": "20e33fbc-d85c-4169-c11e-e5a3e3c7387b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!head food-101/meta/classes.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "apple_pie\n",
            "baby_back_ribs\n",
            "baklava\n",
            "beef_carpaccio\n",
            "beef_tartare\n",
            "beet_salad\n",
            "beignets\n",
            "bibimbap\n",
            "bread_pudding\n",
            "breakfast_burrito\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TDzV_9sYVgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import collections\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Z9mZnvYWh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper method to split dataset into train and test folders\n",
        "from shutil import copy\n",
        "def prepare_data(filepath, src,dest):\n",
        "  classes_images = defaultdict(list)\n",
        "  with open(filepath, 'r') as txt:\n",
        "      paths = [read.strip() for read in txt.readlines()]\n",
        "      for p in paths:\n",
        "        food = p.split('/')\n",
        "        classes_images[food[0]].append(food[1] + '.jpg')\n",
        "\n",
        "  for food in classes_images.keys():\n",
        "    print(\"\\nCopying images into \",food)\n",
        "    if not os.path.exists(os.path.join(dest,food)):\n",
        "      os.makedirs(os.path.join(dest,food))\n",
        "    for i in classes_images[food]:\n",
        "      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
        "  print(\"Copying Done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtgPgVvoYXpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n",
        "print(\"Creating train data...\")\n",
        "prepare_data('food-101/meta/train.txt', 'food-101/images', 'food-101/train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUVgPASnYYuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\n",
        "print(\"Creating test data...\")\n",
        "prepare_data('food-101/meta/test.txt', 'food-101/images', 'food-101/test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kK5Hy2AYZ6E",
        "colab_type": "code",
        "outputId": "932fa54d-fd26-4b94-9d1e-d1f68672c640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Check how many files are in the train folder\n",
        "print(\"Total number of samples in train folder\")\n",
        "!find food-101/train -type d -or -type f -printf '.' | wc -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of samples in train folder\n",
            "75750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w1P_cWcYatn",
        "colab_type": "code",
        "outputId": "5f797130-64ec-4a94-92a9-b2ba3abd28ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Check how many files are in the test folder\n",
        "print(\"Total number of samples in test folder\")\n",
        "!find food-101/test -type d -or -type f -printf '.' | wc -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of samples in test folder\n",
            "25250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN5RTVkJpa7R",
        "colab_type": "code",
        "outputId": "aa8a94a4-2e2d-4385-b7d3-9966ae5b0069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3B0dPotYb1Y",
        "colab_type": "code",
        "outputId": "e6e2cd91-b352-4e46-a2ee-3bff2951d28f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "for layer in vgg_conv.layers[:-4]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# for layer in vgg_conv.layers:\n",
        "#   print(layer, layer.trainable)\n",
        "  \n",
        "model = models.Sequential()\n",
        " \n",
        "model.add(vgg_conv)\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(layers.Dense(4096, activation='relu'))\n",
        "# model.add(layers.Dense(1000, activation='relu'))\n",
        "# model.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(101,kernel_regularizer=regularizers.l2(0.005), activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 7s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 4096)              16384     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 101)               413797    \n",
            "=================================================================\n",
            "Total params: 134,690,725\n",
            "Trainable params: 127,047,269\n",
            "Non-trainable params: 7,643,456\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGD91cyWYdgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "# from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "model = keras.models.load_model('best_model_101class_school2.hdf5')\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg62yqM_Ye4m",
        "colab_type": "code",
        "outputId": "27942fbc-0946-48ac-dcf4-fbcce2c08f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1451
        }
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 128\n",
        "val_batchsize = 64\n",
        "train_dir = 'food-101/train'\n",
        "validation_dir = 'food-101/test'\n",
        "n_classes = 101\n",
        "image_size = 224\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "tbCallBack = TensorBoard(log_dir=LOG_DIR, \n",
        "                         histogram_freq=0,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=32,\n",
        "                         write_images=True,\n",
        "                         update_freq='epoch')\n",
        "\n",
        "# Compile the model\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=optimizers.RMSprop(lr=1e-3),\n",
        "#               metrics=['acc'])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.Adam(lr=1e-3),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
        "#               metrics=['acc'])\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='best_model_101class_school3.hdf5', verbose=1, save_best_only=True)\n",
        "csv_logger = CSVLogger('history3.log', separator='/', append=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1,\n",
        "      callbacks=[csv_logger, checkpointer, tbCallBack])\n",
        " \n",
        "# Save the model\n",
        "model.save('best_model_101class_school3.hdf5')\n",
        "\n",
        "shutil.copy(\"/content/history3.log\", \"/gdrive/My Drive\")\n",
        "shutil.copy(\"/content/best_model_101class_school3.hdf5\", \"/gdrive/My Drive\")\n",
        "\n",
        "# files.download('best_model_101class.hdf5')\n",
        "# files.download('history.log')\n",
        "\n",
        "# source_path = \"/content/best_model_101class_school.hdf5\"\n",
        "# target_path = \"/gdrive/My Drive\"\n",
        "\n",
        "# if not os.path.exists(target_path):\n",
        "#     shutil.copy(source_path, target_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 75750 images belonging to 101 classes.\n",
            "Found 25250 images belonging to 101 classes.\n",
            "Epoch 1/20\n",
            "592/591 [==============================] - 1374s 2s/step - loss: 2.9987 - acc: 0.3008 - val_loss: 3.1998 - val_acc: 0.2962\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.19981, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 2/20\n",
            "592/591 [==============================] - 1345s 2s/step - loss: 2.9979 - acc: 0.3067 - val_loss: 2.7115 - val_acc: 0.3563\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.19981 to 2.71154, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 3/20\n",
            "592/591 [==============================] - 1353s 2s/step - loss: 2.6531 - acc: 0.3707 - val_loss: 2.5523 - val_acc: 0.3965\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.71154 to 2.55234, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 4/20\n",
            "592/591 [==============================] - 1346s 2s/step - loss: 2.6306 - acc: 0.3788 - val_loss: 2.3445 - val_acc: 0.4359\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.55234 to 2.34454, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 5/20\n",
            "592/591 [==============================] - 1347s 2s/step - loss: 2.6950 - acc: 0.3672 - val_loss: 4.4147 - val_acc: 0.2633\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 2.34454\n",
            "Epoch 6/20\n",
            "592/591 [==============================] - 1347s 2s/step - loss: 2.6402 - acc: 0.3764 - val_loss: 2.1913 - val_acc: 0.4670\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.34454 to 2.19126, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 7/20\n",
            "592/591 [==============================] - 1345s 2s/step - loss: 2.4198 - acc: 0.4238 - val_loss: 2.0560 - val_acc: 0.4954\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.19126 to 2.05604, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 8/20\n",
            "592/591 [==============================] - 1344s 2s/step - loss: 2.3115 - acc: 0.4500 - val_loss: 2.0089 - val_acc: 0.5125\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.05604 to 2.00894, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 9/20\n",
            "592/591 [==============================] - 1339s 2s/step - loss: 2.2015 - acc: 0.4754 - val_loss: 1.9630 - val_acc: 0.5301\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.00894 to 1.96305, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 10/20\n",
            "592/591 [==============================] - 1340s 2s/step - loss: 2.1876 - acc: 0.4822 - val_loss: 1.9956 - val_acc: 0.5217\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.96305\n",
            "Epoch 11/20\n",
            "592/591 [==============================] - 1342s 2s/step - loss: 2.1037 - acc: 0.5032 - val_loss: 1.9088 - val_acc: 0.5461\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.96305 to 1.90880, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 12/20\n",
            "592/591 [==============================] - 1344s 2s/step - loss: 2.0282 - acc: 0.5213 - val_loss: 1.9049 - val_acc: 0.5473\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.90880 to 1.90491, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 13/20\n",
            "592/591 [==============================] - 1341s 2s/step - loss: 2.1154 - acc: 0.5061 - val_loss: 2.3580 - val_acc: 0.4704\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.90491\n",
            "Epoch 14/20\n",
            "592/591 [==============================] - 1337s 2s/step - loss: 2.0032 - acc: 0.5278 - val_loss: 1.8339 - val_acc: 0.5644\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.90491 to 1.83390, saving model to best_model_101class_school3.hdf5\n",
            "Epoch 15/20\n",
            "592/591 [==============================] - 1338s 2s/step - loss: 2.5393 - acc: 0.4179 - val_loss: 6.9695 - val_acc: 0.1904\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.83390\n",
            "Epoch 16/20\n",
            "592/591 [==============================] - 1336s 2s/step - loss: 2.2644 - acc: 0.4753 - val_loss: 1.8673 - val_acc: 0.5584\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.83390\n",
            "Epoch 17/20\n",
            "592/591 [==============================] - 1333s 2s/step - loss: 2.6680 - acc: 0.3935 - val_loss: 4.1639 - val_acc: 0.2728\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.83390\n",
            "Epoch 18/20\n",
            "592/591 [==============================] - 1336s 2s/step - loss: 2.5864 - acc: 0.4050 - val_loss: 6.1500 - val_acc: 0.1743\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.83390\n",
            "Epoch 19/20\n",
            "592/591 [==============================] - 1328s 2s/step - loss: 2.5386 - acc: 0.4110 - val_loss: 2.6144 - val_acc: 0.4253\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.83390\n",
            "Epoch 20/20\n",
            "592/591 [==============================] - 1335s 2s/step - loss: 2.2387 - acc: 0.4760 - val_loss: 2.3102 - val_acc: 0.4817\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.83390\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/My Drive/best_model_101class_school3.hdf5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwAOnpXhP-JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7XFwuGQG2hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.copy(\"/content/history2.log\", \"/gdrive/My Drive\")\n",
        "# shutil.copy(\"/content/best_model_101class_school.hdf5\", \"/gdrive/My Drive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKzzXbjBYqsn",
        "colab_type": "code",
        "outputId": "aac9d5f5-d1dc-44d1-dfb3-6a64d78b6f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_accuracy(history,title):\n",
        "    plt.title(title)\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n",
        "    plt.show()\n",
        "def plot_loss(history,title):\n",
        "    plt.title(title)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train_loss', 'validation_loss'], loc='best')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_accuracy(history,'FOOD101--Vggnet16_SGD')\n",
        "plot_loss(history,'FOOD101--Vggnet16_SGD')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d51ba78acb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mplot_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FOOD101--Vggnet16_SGD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FOOD101--Vggnet16_SGD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3kk1n9aG8O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}